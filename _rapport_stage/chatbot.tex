
\part{Un chatbot}

Le but initiale d'un chatbot est un agent pouvant dialoguer avec un utilisateur dans un certain contexte, un contexte est un ensemble de mots donnant lieu à un langage.\linebreak
Un modèle très basique de chatbot est une simplement un programme qui récupère clavier de l'utilisateur puis affiche une réponse formaté, les chatbot basique sont pas aussi intelligent, le contexte de la conversation a besoin d'être conservé pour pouvoir donner un meilleur sens et une interaction bien plus intéressante avec l'utilisateur.\linebreak
\\
Pour qu'un chatbot soit intéressant, celui-ci doit savoir de quoi on parle et savoir donner des réponses pertinentes, sans compter sur la qualité des réponses qui doivent être humainement compréhensible.\linebreak
\linebreak
Dans cette partie, nous allons voir les technologies utilisées, les algorithmes utilisées, des papiers théoriques et pratiques des recherches sur certains algorithmes utilisées et l'assemblage de tout ses composant pour aboutir à un prototype du projet final.

\chapter{Le langage naturelle}

Le langage naturelle n'est pas aussi facile à interprété que une série de valeurs pour les algorithmes de machine learning, c'est pour cela que pour gérer le langage naturelle, nous avons d'autres algorithmes que ce cité ci dessus.
Nous parlerons de \textit{Sequence To Sequence}, \textit{Long Short Term Memory},  \textit{Conditional Random Field}, \textit{Named Entities Recognition} ou de réseaux bayésiens naïf.\\

\pagebreak
\section{Les réseaux bayésiens naif}

C'est la première référence que nous avons quand on cherche dans le registre du travail avec les chaines textuelles.\\
L'idée du réseau bayésiens naïf est de représenter un ensemble de documents en une liste de fréquences de pairs $(w, |w|)$, w était un mot dans le langage des documents concerné, pour chaque labels, nous allons construire un modèle de probabilité via la formule suivante:
\formula{$P(X|Y=y)$}
Pour classifier un document, nous allons utilisé la formule ci dessous et retirer le meilleur résultat en tant que prédiction:
\formula{$ pred = P(X|y) * P(y)$}

D'après le module \textit{Scikit-Learn}, la variante Multinomial se démarque des autres variantes pour des raison de fonctionnement, la où les autres variantes demandent des cardinaux des mots, le multinomial fonctionne avec l'algorithme nommé \textit{TF-IDF}, cette algorithme sera introduit sous peu.\linebreak
\linebreak
Prenons ses deux corpus suivant, extrait de wikipedia. (le label de chaque corpus est représenté par un mot en gras):\\
\begin{description}
\item[Pomme]: La pomme est un fruit comestible à pépins d'un goût sucré et acidulé et à la propriété plus ou moins astringente selon les variétés. D'un point de vue botanique, il s'agit d'un faux-fruit. Elle est produite par les pommiers.
\item[Automobile]: Le terme populaire automobile désigne un véhicule à roues mû par un moteur et destiné au transport terrestre de personnes et de biens.
\end{description}

\pagebreak

La procédure d'apprentissage par Multinomial demande deux traitement sur la donnée, la première est nommé $Tokenization$ et la seconde $Frequencies$:\\
\begin{description}
\item[$Tokenization$]: Pour une chaine textuelle, nous allons supprimer tout les mots dit $stopwords$ comme $'le','la','ces',...$ des mots qui n'ont aucun impacte sur le sens général du texte, si on applique à $\textbf{Automobile}$ ceci donnerai: 
\begin{description}
\item[] terme populaire automobile désigne véhicule roues moteur destiné transport terrestre personnes.
\end{description}

\item[$Frequencies$]: Pour chaque mots présent dans le texte courant, nous allons lui associer un un dans son vecteur, la valeur vide étant zero. Pour que le tableau ne soit pas très large, nous allons prendre ses deux textes suivant: 
\begin{description}
\item[1:] gare train gauche gauche magasin
\item[2:] chaussures rangé haut magasin
\end{description}
Ce qui donne:\\\\
\begin{tabular}{c|c|c|c|c|c|c|c}
$-$ & chaussures & gare & gauche & haut & magasin & rangé & train\\
\hline
1: & 0 & 1 & 2 & 0 & 1 & 0 & 1\\
2: & 1 & 0 & 0 & 1 & 1 & 1 & 0\\
\end{tabular}
\end{description}

\ \linebreak
Pour avoir une implémentation complète venant de $scikit-learn$, la $tokenization$ sera décrit par l'algorithme $CountVectorizer$ et la $Frequencies$ par $TF-idf$.\\
\pagebreak

\section{Conditional Random Field}
Un autre modèle statistique, mais qui cette fois ne s'arrête pas à un simple encodage des variables, mais prend aussi en compte les variables voisines, pour correctement illustrer, prenons trois mots d'une phrase, comme $"pour$ $correctement$ $illustrer"$, et prenons le mot $"correctement"$, ce mot sera inséré dans une structure de données ayant comme champs:
\begin{description}
\item[lower]: le mot en minuscule (donc $"correctement"$)
\item[digit]: un boolean disant si le mot est un nombre ou pas
\item[title]: si ce mot est un titre (sous la forme capitalisé)
\item[trois champs -1]: qui lui contient les trois champs du dessus mais avec le mot $n-1$ 
\item[trois champs +1]: qui lui contient les trois champs du dessus mais avec le mot $n+1$ 
\end{description}

Ce modèle largement utilisé lorsque qu'on veut traiter le langage naturelle donne d'assez bon résultats.\\
\linebreak
Pour appuyer la qualité des résultats, en 2018, l'entreprise Google a développé un algorithme nommé \textbf{BERT} un outils de pré traitement du langage naturelle qui a significativement amélioré les algorithmes de traitement du langage naturelle. Dans le cadre du CRF nous utilisons le mot $n+/-1$ pour $n$, la où \textbf{BERT} prend en compte aussi les mots $n+/-2$, mais ce n'est pas tout, quand l'algorithme devait être testé, celui-ci devait deviner un mot en index $n$ de la phrase en ayant que ses 4 mots, les résultats était correct sans plus, donc en amélioration ils ont finalement laisser le pouvoir à l'algorithme de pouvoir lire la phrase en entier du sens normal (de gauche à droite) et aussi de lire la phrase de droite à gauche, ainsi de considérablement augmenter la précision des réponses.